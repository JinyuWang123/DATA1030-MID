{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b02a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681744c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf=pd.read_csv('C:/Users/wjyjy/Downloads/WA_Fn-UseC_-Telco-Customer-Churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of df\n",
    "customerDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb66acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "customerDf.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(customerDf).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in customerDf.columns:\n",
    "    test=customerDf.loc[:,x].value_counts()\n",
    "    print('The number of rows of {0}：{1}'.format(x,test.sum()))\n",
    "    print('The type of {0}：{1}'.format(x,customerDf[x].dtypes))\n",
    "    print('The context of {0}：\\n{1}\\n'.format(x,test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3536bb6",
   "metadata": {},
   "source": [
    "- We found 11 users with missing data in the \"TotalCharges\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9913f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf['TotalCharges']=customerDf['TotalCharges'].apply(lambda x: np.NaN if str(x).isspace() else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d837a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf['TotalCharges']=customerDf[['TotalCharges']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7790030",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(customerDf).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf20e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customerDf[customerDf['TotalCharges']!=customerDf['TotalCharges']][['tenure','MonthlyCharges','TotalCharges','Churn']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871e998",
   "metadata": {},
   "source": [
    "- We found that these 11 users have a 'tenure' (length of time on the network) of 0, presumably they are new to the network in the current month. Even if the users lost in the month of registration, they still need to pay the current month's fee. Therefore, we change the length of time these 11 users have been on the network to 1, and fill the total consumption amount with the monthly consumption amount, which is in line with the actual situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2399e4ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf=customerDf.fillna(method='pad',axis=1)\n",
    "customerDf['TotalCharges']=customerDf[['TotalCharges']].astype(float)\n",
    "customerDf['SeniorCitizen']=customerDf[['SeniorCitizen']].astype(int)\n",
    "customerDf['tenure']=customerDf[['tenure']].astype(int)\n",
    "customerDf['MonthlyCharges']=customerDf[['MonthlyCharges']].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9599e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customerDf[customerDf['tenure']==0][['tenure','MonthlyCharges','TotalCharges']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0348320b",
   "metadata": {},
   "source": [
    "- View the number and percentage of churned users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePic(name):\n",
    "    plt.savefig('C:/Users/wjyjy/Downloads/{0}.png'.format(name), dpi=600,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed196d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=6,6\n",
    "plt.pie(customerDf['Churn'].value_counts(),labels=customerDf['Churn'].value_counts().index,autopct='%1.2f%%',explode=(0.1,0))\n",
    "plt.title('Churn(Yes/No) Ratio')\n",
    "savePic('Churn(Yes or No) Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "churnDf=customerDf['Churn'].value_counts().to_frame()\n",
    "x=churnDf.index\n",
    "y=churnDf['Churn']\n",
    "\n",
    "plt.bar(x,y,width = 0.5,color = 'c')\n",
    "plt.title('Churn(Yes/No) Num')\n",
    "for a,b in zip(x,y):\n",
    "    plt.text(a,b+10,'%.0f' % b, ha='center', va= 'bottom')\n",
    "savePic('Churn(Yes or No) Num')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f20149",
   "metadata": {},
   "source": [
    "- It is an unbalanced data set, with 26.54% of churned users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb13076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot_percentages(feature,orient='v',axis_name=\"percentage of customers\"):\n",
    "    ratios = pd.DataFrame()\n",
    "    g = (customerDf.groupby(feature)[\"Churn\"].value_counts()/len(customerDf)).to_frame()\n",
    "    g.rename(columns={\"Churn\":axis_name},inplace=True)\n",
    "    g.reset_index(inplace=True)\n",
    "\n",
    "    #print(g)\n",
    "    if orient == 'v':\n",
    "        ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient)\n",
    "        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n",
    "        plt.rcParams.update({'font.size': 13})\n",
    "        #plt.legend(fontsize=10)\n",
    "    else:\n",
    "        ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient)\n",
    "        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n",
    "        #plt.legend(fontsize=10)\n",
    "    plt.title('Churn(Yes/No) Ratio as {0}'.format(feature))\n",
    "    savePic('Churn(Yes or No) Ratio as {0}'.format(feature))\n",
    "    plt.show()\n",
    "barplot_percentages(\"SeniorCitizen\")\n",
    "barplot_percentages(\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f884738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf['churn_rate'] = customerDf['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n",
    "g = sns.FacetGrid(customerDf, col=\"SeniorCitizen\", height=4, aspect=.9)\n",
    "ax = g.map(sns.barplot, \"gender\", \"churn_rate\", palette = \"Blues_d\", order= ['Female', 'Male'])\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "savePic('Churn(Yes or No) Ratio as gender and SeniorCitizen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d9359",
   "metadata": {},
   "source": [
    "- User churn is independent of gender; older users account for a significantly higher percentage of churn than younger users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dcd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(12,4))\n",
    "axis[0].set_title(\"Has Partner\")\n",
    "axis[1].set_title(\"Has Dependents\")\n",
    "axis_y = \"percentage of customers\"\n",
    "\n",
    "# Plot Partner column\n",
    "gp_partner = (customerDf.groupby('Partner')[\"Churn\"].value_counts()/len(customerDf)).to_frame()\n",
    "gp_partner.rename(columns={\"Churn\": axis_y}, inplace=True)\n",
    "gp_partner.reset_index(inplace=True)\n",
    "ax1 = sns.barplot(x='Partner', y= axis_y, hue='Churn', data=gp_partner, ax=axis[0])\n",
    "ax1.legend(fontsize=10)\n",
    "\n",
    "\n",
    "# Plot Dependents column\n",
    "gp_dep = (customerDf.groupby('Dependents')[\"Churn\"].value_counts()/len(customerDf)).to_frame()\n",
    "#print(gp_dep)\n",
    "gp_dep.rename(columns={\"Churn\": axis_y} , inplace=True)\n",
    "#print(gp_dep)\n",
    "gp_dep.reset_index(inplace=True)\n",
    "#print(gp_dep)\n",
    "\n",
    "ax2 = sns.barplot(x='Dependents', y= axis_y, hue='Churn', data=gp_dep, ax=axis[1])\n",
    "\n",
    "\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "ax2.legend(fontsize=10)\n",
    "\n",
    "\n",
    "savePic('Churn(Yes or No) Ratio as partner and dependents')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b925dff",
   "metadata": {},
   "source": [
    "- The churn rate of users with partners is lower than that of users without partners; the churn rate of users with dependents is lower than that of users without dependents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17b6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel density estimaton\n",
    "def kdeplot(feature,xlabel):\n",
    "    plt.figure(figsize=(9, 4))\n",
    "    plt.title(\"KDE for {0}\".format(feature))\n",
    "    ax0 = sns.kdeplot(customerDf[customerDf['Churn'] == 'No'][feature].dropna(), color= 'navy', label= 'Churn: No', shade='True')\n",
    "    ax1 = sns.kdeplot(customerDf[customerDf['Churn'] == 'Yes'][feature].dropna(), color= 'orange', label= 'Churn: Yes',shade='True')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.legend(fontsize=10)\n",
    "kdeplot('tenure','tenure')\n",
    "savePic('Churn(Yes or No) Ratio as tenure kde')\n",
    "kdeplot('MonthlyCharges','MonthlyCharges')\n",
    "savePic('Churn(Yes or No) Ratio as MonthlyCharges kde')\n",
    "kdeplot('TotalCharges','TotalCharges')\n",
    "savePic('Churn(Yes or No) Ratio as TotalCharges kde')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c175b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4.5))\n",
    "barplot_percentages(\"InternetService\", orient=\"h\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325926c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"PhoneService\",\"MultipleLines\",\"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\"]\n",
    "df1 = pd.melt(customerDf[customerDf[\"InternetService\"] != \"No\"][cols])\n",
    "df1.rename(columns={'value': 'Has service'},inplace=True)\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.countplot(data=df1, x='variable', hue='Has service')\n",
    "ax.set(xlabel='Internet Additional service', ylabel='Num of customers')\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.legend( labels = ['No Service', 'Has Service'],fontsize=15)\n",
    "plt.title('Num of Customers as Internet Additional Service')\n",
    "savePic('Churn(Yes or No) Num as Internet Additional Service')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19351b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "df1 = customerDf[(customerDf.InternetService != \"No\") & (customerDf.Churn == \"Yes\")]\n",
    "df1 = pd.melt(df1[cols])\n",
    "df1.rename(columns={'value': 'Has service'}, inplace=True)\n",
    "ax = sns.countplot(data=df1, x='variable', hue='Has service', hue_order=['No', 'Yes'])\n",
    "ax.set(xlabel='Internet Additional service', ylabel='Churn Num')\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.legend( labels = ['No Service', 'Has Service'],fontsize=15)\n",
    "plt.title('Num of Churn Customers as Internet Additional Service')\n",
    "savePic('Churn Num as Internet Additional Service')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12d3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(customerDf, col=\"PaperlessBilling\", height=6, aspect=.9)\n",
    "ax = g.map(sns.barplot, \"Contract\", \"churn_rate\", palette = \"Blues_d\", order= ['Month-to-month', 'One year', 'Two year'])\n",
    "plt.rcParams.update({'font.size':18})\n",
    "savePic('Churn Ratio as PaperlessBilling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d74398",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4.5))\n",
    "barplot_percentages(\"MultipleLines\", orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 4.5))\n",
    "barplot_percentages(\"PaymentMethod\",orient='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95f486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf = customerDf.drop(['customerID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9546b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf = customerDf.drop(['PhoneService'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce16930",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.loc[:,'OnlineSecurity'].replace(to_replace='No internet service',value='No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94251e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.loc[:,'OnlineBackup'].replace(to_replace='No internet service',value='No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29087a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.loc[:,'DeviceProtection'].replace(to_replace='No internet service',value='No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d6b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.loc[:,'TechSupport'].replace(to_replace='No internet service',value='No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbd20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.loc[:,'StreamingMovies'].replace(to_replace='No internet service',value='No',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf = customerDf.drop(['churn_rate'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d03089",
   "metadata": {},
   "source": [
    "## SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfafd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in customerDf.columns:\n",
    "    test=customerDf.loc[:,x].value_counts()\n",
    "    print('The number of rows of {0}：{1}'.format(x,test.sum()))\n",
    "    print('The type of {0}：{1}'.format(x,customerDf[x].dtypes))\n",
    "    print('The context of {0}：\\n{1}\\n'.format(x,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e134062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc81a5",
   "metadata": {},
   "source": [
    "## PREPROCCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfec80d",
   "metadata": {},
   "source": [
    "- Looking at the data types, we find that except for \"tenure\", \"MonthlyCharges\", and \"TotalCharges\", which are continuous features, all the others are category features. For continuous features, standardscaler is better to use. For category features, one-hot encoding is used;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79204c02",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f9b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost\n",
    "import warnings\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import shap\n",
    "import pickle\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d221a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = customerDf['Churn']\n",
    "X = customerDf.loc[:, customerDf.columns != 'Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f1c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onehot_ftrs = ['gender','SeniorCitizen', 'Partner', 'Dependents', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod']\n",
    "\n",
    "std_ftrs = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# collect all the encoders into one preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "prep = Pipeline(steps=[('preprocessor', preprocessor)]) # for now we only preprocess, later we will add other steps here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d8ae3",
   "metadata": {},
   "source": [
    "## Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "test_set = []\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=42*i)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=42*i)\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "    \n",
    "    test_set.append(pd.concat([pd.DataFrame(X_test_prep,columns=preprocessor.get_feature_names_out()),pd.DataFrame(np.reshape(np.array(y_test), (1, -1)).ravel(),columns=['y_true'])],axis=1))\n",
    "    \n",
    "    \n",
    "    param_grid = {\n",
    "                  'C':  [1e-3,1e-2,1e-1,1,1e+3,1e+2,1e+1],\n",
    "                  'max_iter':[10,100]             \n",
    "                  } \n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score_f1 = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) # initialize the classifier\n",
    "        clf = LogisticRegression(**params,penalty='l2', solver='liblinear',random_state = 42*i)\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        val_score_f1[p] = f1_score(y_val, y_val_pred,pos_label='Yes')\n",
    "        print('   ',train_score[p],val_score[p],val_score_f1[p])\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation accuracy score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test accuracy score:',test_scores[i])\n",
    "    print('test F1 score:', f1_score(y_test, y_test_pred,pos_label='Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475d5175",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nr_states):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    testset_pred.append(pd.DataFrame(np.reshape(np.array(y_pred), (1, -1)).ravel(),columns=['y_true']))\n",
    "testset_pred = []\n",
    "file = open('results/ridge.save', 'wb')\n",
    "pickle.dump((final_models,test_set,testset_pred),file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fde0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of F1 score:',np.mean(val_score_f1))\n",
    "print('mean of accuracy:',np.mean(val_score))\n",
    "print('std of F1 score:',np.std(val_score_f1))\n",
    "print('std of accuracy:',np.std(val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1236375",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set=pd.DataFrame()\n",
    "\n",
    "for i in range(0,1):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    df_subset=pd.concat([test_set[i],pd.DataFrame(y_pred,columns=['y_pred'])],axis=1)\n",
    "    final_set=pd.concat([final_set,df_subset])\n",
    "cm = confusion_matrix(final_set['y_true'],final_set['y_pred'])\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label',fontsize=12)\n",
    "plt.xlabel('Predicted Label',fontsize=12)\n",
    "plt.title('Confusion matrix (Ridge)',fontsize=15)\n",
    "savePic('Confusion matrix (Ridge)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fc32c3",
   "metadata": {},
   "source": [
    "## Lasso Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=42*i)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=42*i)\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "    \n",
    "    test_set.append(pd.concat([pd.DataFrame(X_test_prep,columns=preprocessor.get_feature_names_out()),pd.DataFrame(np.reshape(np.array(y_test), (1, -1)).ravel(),columns=['y_true'])],axis=1))\n",
    "    \n",
    "    param_grid = {\n",
    "                  'C':  [1e-3,1e-2,1e-1,1,1e+3,1e+2,1e+1],\n",
    "                  'max_iter':[10,100]             \n",
    "                  } \n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score_f1 = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) # initialize the classifier\n",
    "        clf = LogisticRegression(**params,penalty='l1', solver='liblinear',random_state = 42*i)\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        val_score_f1[p] = f1_score(y_val, y_val_pred,pos_label='Yes')\n",
    "        print('   ',train_score[p],val_score[p],val_score_f1[p])\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n",
    "    print('test F1 score:', f1_score(y_test, y_test_pred,pos_label='Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906e814",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nr_states):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    testset_pred.append(pd.DataFrame(np.reshape(np.array(y_pred), (1, -1)).ravel(),columns=['y_true']))\n",
    "testset_pred = []\n",
    "file = open('results/Lasso.save', 'wb')\n",
    "pickle.dump((final_models,test_set,testset_pred),file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of F1 score:',np.mean(val_score_f1))\n",
    "print('mean of accuracy:',np.mean(val_score))\n",
    "print('std of F1 score:',np.std(val_score_f1))\n",
    "print('std of accuracy:',np.std(val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f213dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set=pd.DataFrame()\n",
    "\n",
    "for i in range(0,1):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    df_subset=pd.concat([test_set[i],pd.DataFrame(y_pred,columns=['y_pred'])],axis=1)\n",
    "    final_set=pd.concat([final_set,df_subset])\n",
    "cm = confusion_matrix(final_set['y_true'],final_set['y_pred'])\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label',fontsize=12)\n",
    "plt.xlabel('Predicted Label',fontsize=12)\n",
    "plt.title('Confusion matrix (Lasso)',fontsize=15)\n",
    "savePic('Confusion matrix (Lasso)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c9f51",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c932274b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=42*i)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=42*i)\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "    \n",
    "    test_set.append(pd.concat([pd.DataFrame(X_test_prep,columns=preprocessor.get_feature_names_out()),pd.DataFrame(np.reshape(np.array(y_test), (1, -1)).ravel(),columns=['y_true'])],axis=1))\n",
    "    \n",
    "    param_grid = {\n",
    "                  'max_depth': [7,8,9,10,11,12,13,14], # no upper bound so the values are evenly spaced in log\n",
    "                  'max_features': [0.25, 0.5,0.75,1.0] # linearly spaced because it is between 0 and 1, 0 is omitted\n",
    "                  } \n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score_f1 = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) \n",
    "        clf = RandomForestClassifier(**params,n_jobs=-1,random_state=42*i) # initialize the classifier\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        val_score_f1[p] = f1_score(y_val, y_val_pred,pos_label='Yes')\n",
    "        print('   ',train_score[p],val_score[p],val_score_f1[p])\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n",
    "    print('test F1 score:', f1_score(y_test, y_test_pred,pos_label='Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nr_states):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    testset_pred.append(pd.DataFrame(np.reshape(np.array(y_pred), (1, -1)).ravel(),columns=['y_true']))\n",
    "testset_pred = []\n",
    "file = open('results/RF.save', 'wb')\n",
    "pickle.dump((final_models,test_set,testset_pred),file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of F1 score:',np.mean(val_score_f1))\n",
    "print('mean of accuracy:',np.mean(val_score))\n",
    "print('std of F1 score:',np.std(val_score_f1))\n",
    "print('std of accuracy:',np.std(val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3079b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set=pd.DataFrame()\n",
    "\n",
    "for i in range(0,1):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    df_subset=pd.concat([test_set[i],pd.DataFrame(y_pred,columns=['y_pred'])],axis=1)\n",
    "    final_set=pd.concat([final_set,df_subset])\n",
    "cm = confusion_matrix(final_set['y_true'],final_set['y_pred'])\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label',fontsize=12)\n",
    "plt.xlabel('Predicted Label',fontsize=12)\n",
    "plt.title('Confusion matrix (RF)',fontsize=15)\n",
    "savePic('Confusion matrix (RF)')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aace83aa",
   "metadata": {},
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc23133",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "for i in range(nr_states):\n",
    "    warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "    print('randoms state '+str(i+1))\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=42*i)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=42*i)\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "    \n",
    "    test_set.append(pd.concat([pd.DataFrame(X_test_prep,columns=preprocessor.get_feature_names_out()),pd.DataFrame(np.reshape(np.array(y_test), (1, -1)).ravel(),columns=['y_true'])],axis=1))\n",
    "    \n",
    "    param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [100,1000],\n",
    "              \"max_depth\": [3,5,7,9,11,13,15],\n",
    "                'min_child_weight':[1,5,7,9],\n",
    "                 # 'subsample':[0.5],\n",
    "                'eval_metric':['auc']}\n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score_f1 = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) \n",
    "        clf = xgboost.sklearn.XGBClassifier(**params,random_state = 42*i,n_jobs=-1) # initialize the classifier\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        val_score_f1[p] = f1_score(y_val, y_val_pred,pos_label='Yes')\n",
    "        print('   ',train_score[p],val_score[p],val_score_f1[p])\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n",
    "    print('test F1 score:', f1_score(y_test, y_test_pred,pos_label='Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bc79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nr_states):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    testset_pred.append(pd.DataFrame(np.reshape(np.array(y_pred), (1, -1)).ravel(),columns=['y_true']))\n",
    "testset_pred = []\n",
    "file = open('results/XGB.save', 'wb')\n",
    "pickle.dump((final_models,test_set,testset_pred),file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9502ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of F1 score:',np.mean(val_score_f1))\n",
    "print('mean of accuracy:',np.mean(val_score))\n",
    "print('std of F1 score:',np.std(val_score_f1))\n",
    "print('std of accuracy:',np.std(val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set=pd.DataFrame()\n",
    "\n",
    "for i in range(0,1):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    df_subset=pd.concat([test_set[i],pd.DataFrame(y_pred,columns=['y_pred'])],axis=1)\n",
    "    final_set=pd.concat([final_set,df_subset])\n",
    "cm = confusion_matrix(final_set['y_true'],final_set['y_pred'])\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label',fontsize=12)\n",
    "plt.xlabel('Predicted Label',fontsize=12)\n",
    "plt.title('Confusion matrix (XGBoost)',fontsize=15)\n",
    "savePic('Confusion matrix (XGBoost)')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d604cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = []\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=42)\n",
    "X_train_prep = prep.fit_transform(X_train)\n",
    "X_val_prep = prep.transform(X_val)\n",
    "X_test_prep = prep.transform(X_test)\n",
    "\n",
    "test_set.append(pd.DataFrame(X_test_prep,columns=preprocessor.get_feature_names_out()))\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [100],\n",
    "              \"max_depth\": [5],\n",
    "               # 'min_child_weight':[5],\n",
    "                 # 'subsample':[0.5],\n",
    "                'eval_metric':['auc']}\n",
    "clf = xgboost.sklearn.XGBClassifier(**params,random_state = 42,n_jobs=-1) # initialize the classifier\n",
    "clf.fit(X_train_prep,y_train)\n",
    "clf.feature_importances_\n",
    "test_set = pd.DataFrame(test_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a318e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf)\n",
    "shap_values = explainer.shap_values(test_set)\n",
    "shap.summary_plot(shap_values, test_set, plot_type=\"bar\",max_display=10,show=False)\n",
    "savePic('global1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73108082",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, test_set,max_display=10,show=False)\n",
    "savePic('global2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e916725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "expit(-3.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d935e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "import matplotlib.pyplot as pyplot\n",
    "index = 0\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values[index,:], \n",
    "                test_set.iloc[index,:],show=False,matplotlib=True)\n",
    "savePic('local')\n",
    "#,show=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388d8e9",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477400d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_states = 5\n",
    "test_scores = np.zeros(nr_states)\n",
    "final_models = []\n",
    "test_set = []\n",
    "\n",
    "for i in range(nr_states):\n",
    "    print('randoms state '+str(i+1))\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=42*i)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=42*i)\n",
    "    X_train_prep = prep.fit_transform(X_train)\n",
    "    X_val_prep = prep.transform(X_val)\n",
    "    X_test_prep = prep.transform(X_test)\n",
    "    \n",
    "    test_set.append(pd.concat([pd.DataFrame(X_test_prep,columns=preprocessor.get_feature_names_out()),pd.DataFrame(np.reshape(np.array(y_test), (1, -1)).ravel(),columns=['y_true'])],axis=1))\n",
    "    \n",
    "    param_grid = {\n",
    "                   \n",
    "                  'max_iter':[10,100] ,\n",
    "                    'C': [1e-3,1e-2,1e-1, 1e0, 1e1,1e2] \n",
    "                  } \n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score_f1 = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print('   ',params) \n",
    "        clf = SVC(**params,random_state = 42*i) # initialize the classifier\n",
    "        clf.fit(X_train_prep,y_train) # fit the model\n",
    "        models.append(clf) # save it\n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = clf.predict(X_train_prep)\n",
    "        train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "        y_val_pred = clf.predict(X_val_prep)\n",
    "        val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "        val_score_f1[p] = f1_score(y_val, y_val_pred,pos_label='Yes')\n",
    "        print('   ',train_score[p],val_score[p],val_score_f1[p])\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_models.append(models[np.argmax(val_score)])\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "    test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "    print('test score:',test_scores[i])\n",
    "    print('test F1 score:', f1_score(y_test, y_test_pred,pos_label='Yes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nr_states):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    testset_pred.append(pd.DataFrame(np.reshape(np.array(y_pred), (1, -1)).ravel(),columns=['y_true']))\n",
    "testset_pred = []\n",
    "file = open('results/SVC.save', 'wb')\n",
    "pickle.dump((final_models,test_set,testset_pred),file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('mean of F1 score:',np.mean(val_score_f1))\n",
    "print('mean of accuracy:',np.mean(val_score))\n",
    "print('std of F1 score:',np.std(val_score_f1))\n",
    "print('std of accuracy:',np.std(val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a394b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_set=pd.DataFrame()\n",
    "\n",
    "for i in range(0,1):\n",
    "    y_pred = final_models[i].predict(test_set[i].iloc[:,test_set[i].columns!='y_true'])\n",
    "    df_subset=pd.concat([test_set[i],pd.DataFrame(y_pred,columns=['y_pred'])],axis=1)\n",
    "    final_set=pd.concat([final_set,df_subset])\n",
    "cm = confusion_matrix(final_set['y_true'],final_set['y_pred'])\n",
    "disp = ConfusionMatrixDisplay(cm,display_labels=['class 0', 'class 1'])\n",
    "disp.plot()\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True Label',fontsize=12)\n",
    "plt.xlabel('Predicted Label',fontsize=12)\n",
    "plt.title('Confusion matrix (SVC)',fontsize=15)\n",
    "savePic('Confusion matrix (SVC)')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "tips = sns.load_dataset(\"tips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(1,2,constrained_layout=True, figsize=(8, 3))\n",
    "pic = sns.boxplot(x=\"day\", y=\"total_bill\", data=tips, ax=ax[0])\n",
    "pic.set_title('x=\"day\", y=\"total_bill\"')\n",
    "pic = sns.boxplot(x=\"total_bill\", y=\"day\", data=tips, ax=ax[1])\n",
    "pic.set_title('x=\"total_bill\", y=\"day\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87edf9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
